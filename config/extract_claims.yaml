llm:
  server: "http://localhost:11434"          # Ollama server URL
  model: "mistral:7b"                       # LLM model name
  temperature: 0.1                          # Sampling temperature
  request_timeout: 120                      # HTTP timeout in seconds

run:
  progress_every: 1                         # Print progress every N chunks
  debug: false                               # Print prompts and raw LLM replies

embeddings:
  model_id: "BAAI/bge-small-en-v1.5"        # SentenceTransformer model for claims
  batch_size: 64                            # Batch size for embedding claims
  normalize: true                           # Normalize embedding vectors

sqlite:
  claims_table_name: "claims"               # SQLite claims table name

templates:
  dir: "prompts"                            # Folder with Jinja templates
  prefix: "extract_claims_prefix.j2"        # Template for prefix mode
  rewrite: "extract_claims_rewrite.j2"      # Template for rewrite mode

modes:
  prefix: "prefix"                          # Claims grounded in raw chunk
  rewrite: "rewrite"                        # Claims grounded in contextualized chunk
