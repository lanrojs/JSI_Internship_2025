llm:
  server: "http://localhost:11434"      # Ollama server URL
  model: "mistral:7b"                   # LLM model name
  temperature: 0.1                      # Sampling temperature
  request_timeout: 120                  # Seconds
  max_retries: 3                        # Retry attempts on failure
  retry_wait: 2.0                       # Seconds between retries

embeddings:
  model_id: "BAAI/bge-small-en-v1.5"    # SentenceTransformer model
  normalize: true                       # Normalize embedding vectors

local_context:
  before: 1                             # Number of previous raw chunks
  after: 1                              # Number of following raw chunks

doc_retrieval:
  top_k: 2                              # k_doc: doc-level retrieved chunks
  dedup_against_local: true             # Avoid reusing local neighbors

system_context:
  enabled: true                         # Enable external system corpus
  db_dir: "system_corpus"               # Directory containing *.sqlite
  top_k: 1                              # k_sys: system-level retrieved chunks

relevance_filter:
  enabled: true                         # Use LLM for relevance filtering
  debug: true                           # Print relevance prompt & reply

retrieval_weights:
  dense: 0.6                            # Weight for dense embeddings
  bm25: 0.4                             # Weight for BM25 scores

templates:
  dir: "prompts"                        # Folder with Jinja templates
  prefix: "contextualize_prefix.j2"     # Template for prefix mode
  rewrite: "contextualize_rewrite.j2"   # Template for rewrite mode
  relevance: "relevance_filter.j2"      # Template for relevance filter

sqlite:
  table_name: "contextualized_chunks"   # SQLite table name

modes:
  prefix: "prefix"                      # Prefix mode (prepend context note)
  rewrite: "rewrite"                    # Rewrite mode (self-contained)
