[
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_1",
    "chunk": "Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3√ó bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_2",
    "chunk": "models that are 2-3√ó bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_3",
    "chunk": "Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_4",
    "chunk": "by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50√ó the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_5",
    "chunk": "al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_6",
    "chunk": "and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. ¬© 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_7",
    "chunk": "Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_8",
    "chunk": "be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages. 2. Model Architecture Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_9",
    "chunk": "We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters Non-embedding Parameters 2B 590,118,912 2,024,517,888 9B 917,962,752 8,324,201,984 27B 1,180,237,824 26,047,480,320 Table 2 | Parameter counts for the Gemma models. We inherit from the large Gemini vocabulary (256k entries), that is designed to work on a large number of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages. length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_10",
    "chunk": "length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity (Shazeer, 2020). A few elements differ between Gemma 1 and Gemma 2, including using deeper networks. We summarize the key differences below. Local Sliding Window and Global Attention. We alternate between a local sliding window attention (Beltagy et al., 2020a,b) and global attention (Luong et al., 2015) in every other layer. The sliding window size of local attention layers is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens. Logit soft-capping. We cap logits (Bello et al., 2016) in each attention layer and the final layer such that the value of the logits stays between ‚àísoft_cap and"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_11",
    "chunk": "soft-capping. We cap logits (Bello et al., 2016) in each attention layer and the final layer such that the value of the logits stays between ‚àísoft_cap and +soft_cap. More specifically, we cap the logits with the following function: logits ‚Üê soft_cap ‚àó tanh(logits/soft_cap). We set the soft_cap parameter to 50.0 for the selfattention layers and to 30.0 for the final layer. Post-norm and pre-norm with RMSNorm. To stabilize training, we use RMSNorm (Zhang and Sennrich, 2019) to normalize the input and output of each transformer sub-layer, the attention layer, and the feedforward layer. Grouped-Query Attention (Ainslie et al., 2023). We use GQA with num_groups = 2, based on ablations showing increased speed at inference"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_12",
    "chunk": "layer. Grouped-Query Attention (Ainslie et al., 2023). We use GQA with num_groups = 2, based on ablations showing increased speed at inference time while maintaining downstream performance. 2 Gemma 2: Improving Open Language Models at a Practical Size 3. Pre-training We provide a brief overview of the parts of our pre-training that differs from Gemma 1. 3.1. Training Data We train Gemma 2 27B on 13 trillion tokens of primarily-English data, the 9B model on 8 trillion tokens, and the 2B on 2 trillion tokens. These tokens come from a variety of data sources, including web documents, code, and science articles. Our models are not multimodal and are not trained specifically for state-of-the-art multilingual capabilities. The final data mixture was determined through ablations similar to the approach in Gemini 1.0 (Gemini Team, 2023)"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_13",
    "chunk": "for state-of-the-art multilingual capabilities. The final data mixture was determined through ablations similar to the approach in Gemini 1.0 (Gemini Team, 2023). Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries. Filtering. We use the same data filtering techniques as Gemma 1. Specifically, we filter the pretraining dataset to reduce the risk of unwanted or unsafe utterances, filter out certain personal information or other sensitive data, decontaminate evaluation sets from our pre-training data mixture, and reduce the risk of recitation by minimizing the proliferation of sensitive outputs. Shards Model Type #Chips Data Model 2B TPUv5e 512 512 1 9B TPUv4 4096 1024 4 27B"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_14",
    "chunk": "minimizing the proliferation of sensitive outputs. Shards Model Type #Chips Data Model 2B TPUv5e 512 512 1 9B TPUv4 4096 1024 4 27B TPUv5p 6144 768 8 Table 3 | Training infrastructure with sharding. 3.2. Knowledge Distillation Given a large model used as a teacher, we learn smaller models by distilling from the probability given by the teacher of each token ùë• given its context ùë•ùëê, i.e., ùëÉùëá (ùë• | ùë•ùëê). More precisely, we minimize the negative log-likelihood between the Context Relevant Token User turn user Model turn model Start of conversation turn <start_of_turn> End of conversation turn <end_of_turn> Beginning of sequence <bos> End of sequence <eos> Table 4 | Relevant formatting control tokens used for Gemma models. probabilities from the teacher and the student: min"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_15",
    "chunk": "end_of_turn> Beginning of sequence <bos> End of sequence <eos> Table 4 | Relevant formatting control tokens used for Gemma models. probabilities from the teacher and the student: min ùëÉùëÜ ‚àëÔ∏Å ùë• ‚àíùëÉùëá (ùë• | ùë•ùëê) log ùëÉùëÜ (ùë• | ùë•ùëê), where ùëÉùëÜ is the parameterized probability of the student. Note that knowledge distillation was also used in Gemini 1.5 (Gemini Team, 2024). 3.3. Compute Infrastructure We train our models with TPUv4, TPUv5e, and TPUv5p as outlined in Table 3. For the 2B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding. For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_16",
    "chunk": "data replication and 1-way model sharding. For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips, with 1024-way data replication and 4-way model sharding. For the 27B model, we train on an 8x24x32 configuration of TPUv5p, totaling 6144 chips, with 768-way data replication and 8-way model sharding. The optimizer state is further sharded using techniques similar to ZeRO-3 (Ren et al., 2021). For scales beyond a single pod, we perform a data-replica reduction over the data center network, using the Pathways approach of Barham et al. (2022). We also use the 'single controller' programming paradigm of Jax (Roberts et al., 2023) and Pathways (Barham et al., 2022). As in Gemma"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_17",
    "chunk": "(2022). We also use the 'single controller' programming paradigm of Jax (Roberts et al., 2023) and Pathways (Barham et al., 2022). As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019). 3 Gemma 2: Improving Open Language Models at a Practical Size 3.4. Carbon Footprint We estimate the carbon emissions from pretraining the Gemma models to be 1247.61 ùë°ùê∂ùëÇ2ùëíùëû. As in Gemma 1 (Gemma Team, 2024), this value is calculated based on the hourly energy usage reported directly from our TPU data centers and scaled to account for the additional energy expended to create and maintain the data center. Importantly, Google data centers are carbon neutral, achieved through a combination of energy efficiency, renewable energy purchases, and carbon offsets. This carbon neutrality applies to our experiments and"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_18",
    "chunk": "and maintain the data center. Importantly, Google data centers are carbon neutral, achieved through a combination of energy efficiency, renewable energy purchases, and carbon offsets. This carbon neutrality applies to our experiments and the machines running them. 4. Post-Training For post-training, we fine-tune our pre-trained models into instruction-tuned models. First, we apply supervised fine-tuning (SFT) on a mix of text-only, English-only synthetic and humangenerated prompt-response pairs. We then apply RLHF on top of these models with the reward model trained on labelled English-only preference data and the policy based on the same prompts as the SFT phase. Finally, we average the models obtained after each phase to improve their overall performance. The final data mixtures and post-training recipe, which includes tuned hyperparameters, were chosen on the basis of improving helpfulness while minimizing model harms related to safety and"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_19",
    "chunk": "performance. The final data mixtures and post-training recipe, which includes tuned hyperparameters, were chosen on the basis of improving helpfulness while minimizing model harms related to safety and hallucinations. We extended the post-training data from Gemma 1.1 with a mixture of internal and external public data. In particular, we use the prompts, but not the answers from LMSYS-chat-1M (Zheng et al., 2023). All of our data go through a filtering stage described below. Supervised fine-tuning (SFT). We run behavioral cloning on synthetic and real prompts, and responses predominantly synthetically generated by the teacher, that is a larger model. We also run distillation from the teacher on the student's distribution (Agarwal et al., 2024; Gu et al., 2024). Reinforcement Learning from Human Feedback (RLHF). We use a similar"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_20",
    "chunk": "student's distribution (Agarwal et al., 2024; Gu et al., 2024). Reinforcement Learning from Human Feedback (RLHF). We use a similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) but a different reward model, which is an order of magnitude First turn User: <start_of_turn>user Knock knock.<end_of_turn> <start_of_turn>model Model: Who's there?<end_of_turn><eos> Second turn User: <start_of_turn>user Knock knock.<end_of_turn> <start_of_turn>model Model: Who's there?<end_of_turn> User: <start_of_turn>user Gemma.<end_of_turn> <start_of_turn>model Model: Gemma"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_21",
    "chunk": "Who's there?<end_of_turn> User: <start_of_turn>user Gemma.<end_of_turn> <start_of_turn>model Model: Gemma who?<end_of_turn><eos> Table 5 | Example dialogue with user and model control tokens. To proceed with multi-turn, remove the model-outputted <eos>, add back the usual user turn's control tokens and continue with the following turn's chat template. larger than the policy. The new reward model is also oriented more towards conversational capabilities, specifically multi-turn. Model merging. We average different models obtained by running our pipeline with different hyperparameters (Ram√© et al., 2024). Data filtering. When using synthetic data, we run several stages of filtering to remove examples that show certain personal information, unsafe or toxic model outputs, mistaken self-identification data, and duplicated examples"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_22",
    "chunk": "Data filtering. When using synthetic data, we run several stages of filtering to remove examples that show certain personal information, unsafe or toxic model outputs, mistaken self-identification data, and duplicated examples. Following Gemini, we find that including subsets of data that encourage better in-context attribution, hedging, and refusals to minimize hallucinations improves performance on factuality metrics, without degrading model performance on other metrics. Formatting. Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema. See the dialogue example in Table 5. Notice that the model explicitly ends generations with <end_of_turn><eos> tokens, while previously it only generated <eos>. For the motivation behind this formatting structure, see Gemma 1. 4 Gemma 2: Improving Open Language Models at a Practical Size 5. Ablations In this"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_23",
    "chunk": "it only generated <eos>. For the motivation behind this formatting structure, see Gemma 1. 4 Gemma 2: Improving Open Language Models at a Practical Size 5. Ablations In this section, we focus on the main finding of this work, which is the impact of knowledge distillation on small language models. from scratch distilled Average (3 bench.) 60.3 67.7 Table 6 | Comparison between a 2B model trained over 500B tokens either from scratch or with distillation from a 7B model. Distillation versus from scratch. In Table 6, we show that distilling from a larger model improves performance compared to training from scratch. Note that 500B is 10√ó more than the computeoptimal number of tokens for a 2B model. We distill from a 7B model to keep a ratio similar to our target distillation from 27B to 9B. 200M 400M 1B from scratch 23"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_24",
    "chunk": "a 2B model. We distill from a 7B model to keep a ratio similar to our target distillation from 27B to 9B. 200M 400M 1B from scratch 23 19 17 distilled (7B) 21 17 15 Table 7 | Perplexity measured on a validation set of models of different sizes trained with or without distillation. The teacher has 7B parameters. Impact of distillation w.r.t. model size. In Table 7, we measure the impact of distillation as model size increases. We observe that the gain remains as the model size is scaled. In this ablation, we maintain the size of the teacher at 7B and train smaller models to simulate the same gap as between our final teacher and student sizes. MHA GQA Average (4 bench.) 50.3 50.8 Table 8 | Comparing the impact of replacing MultiHead Attention (MHA) with GQA on a 9B model averaged"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_25",
    "chunk": "MHA GQA Average (4 bench.) 50.3 50.8 Table 8 | Comparing the impact of replacing MultiHead Attention (MHA) with GQA on a 9B model averaged over 4 benchmarks. GQA versus MHA. In Table 8, we compare two instances of our 9B with MHA or GQA. We observe overall few changes in performance between both models as measured on several benchmarks. We choose GQA since it requires fewer parameters and is faster at inference time. Wide versus deep. In Table 9, we show that a deeper 9B network is slightly better than a wider 9B for the same number of parameters. Although the gap is small, it is consistent across benchmarks and warrants the switch to a deeper architecture. Wide Deep Average (4 bench.) 50.8 52.0 Table 9 | Wide versus deep 9B models. Performance on 4 benchmarks, higher is better. Changing sliding window size. In Table 10, we"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_26",
    "chunk": "(4 bench.) 50.8 52.0 Table 9 | Wide versus deep 9B models. Performance on 4 benchmarks, higher is better. Changing sliding window size. In Table 10, we show that we can change the sliding window size of the local attention layers of the models during inference with moderate impact on perplexity. Adjusting the size of the sliding window can thus be a leverage for slight inference speed gain. sliding window 4096 2048 1024 perplexity (val. set) 1.63 1.63 1.64 Table 10 | Impact of changing the sliding window size at inference time for the 9B model. Impact of formatting. We measure performance variance on MMLU across prompt/evaluation formatting variations. Table 11 shows the standard deviations of MMLU scores for 12 formatting/evaluation combinations, a proxy for undesired performance variability. The Gemma 2B models are slightly less format-robust than the larger ones. Notably, Mistral"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_27",
    "chunk": "MMLU scores for 12 formatting/evaluation combinations, a proxy for undesired performance variability. The Gemma 2B models are slightly less format-robust than the larger ones. Notably, Mistral 7B is significantly less robust than our models. Standard Deviation Gemma 1 2B 1.5 Gemma 2 2B 2.1 Mistral 7B 6.9 Gemma 1 7B 0.7 Gemma 2 9B 0.9 Gemma 2 27B 1.0 Table 11 | Standard deviations of MMLU scores for 12 combinations of formatting and evaluation"
  }
]