[
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_1",
    "chunk": "Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_2",
    "chunk": "models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_3",
    "chunk": "Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_4",
    "chunk": "by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_5",
    "chunk": "al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_6",
    "chunk": "and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_7",
    "chunk": "4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_8",
    "chunk": "be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages. 2. Model Architecture Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters"
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_9",
    "chunk": "2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters Non-embedding Parameters 2B 590,118,912 2,024,517,888 9B 917,962,752 8,324,201,984 27B 1,180,237,824 26,047,480,320 Table 2 | Parameter counts for the Gemma models. We inherit from the large Gemini vocabulary (256k entries), that is designed to work on a large number of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages. length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity"
  }
]