{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_1", "claim_id": "gemma2_sample_1#1", "claim_text": "Gemma 2 models range in scale from 2 billion to 27 billion parameters.", "source_quote": "ranging in scale from 2 billion to 27 billion parameters"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_1", "claim_id": "gemma2_sample_1#2", "claim_text": "Gemma 2 applies interleaving local-global attentions (Beltagy et al., 2020a).", "source_quote": "such as interleaving local-global attentions (Beltagy et al., 2020a)"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_1", "claim_id": "gemma2_sample_1#3", "claim_text": "Gemma 2 applies group-query attention (Ainslie et al., 2023).", "source_quote": "such as group-query attention (Ainslie et al., 2023)"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_1", "claim_id": "gemma2_sample_1#4", "claim_text": "The 2B and 9B models are trained with knowledge distillation (Hinton et al., 2015).", "source_quote": "We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015)"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_1", "claim_id": "gemma2_sample_1#5", "claim_text": "The resulting models deliver the best performance for their size.", "source_quote": "The resulting models deliver the best performance for their size"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_1", "claim_id": "gemma2_sample_1#6", "claim_text": "Gemma 2 models are released to the community.", "source_quote": "We release all our models to the community"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_2", "claim_id": "gemma2_sample_2#1", "claim_text": "Models that are 2-3× bigger have demonstrated unprecedented performance on reasoning benchmarks.", "source_quote": "The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024)."}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_2", "claim_id": "gemma2_sample_2#2", "claim_text": "Newest large models demonstrate multimodal capabilities.", "source_quote": "The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024)."}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_2", "claim_id": "gemma2_sample_2#3", "claim_text": "Newest large models demonstrate multilingual capabilities.", "source_quote": "The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024)."}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_2", "claim_id": "gemma2_sample_2#4", "claim_text": "Newest large models can use context lengths of over 1M tokens.", "source_quote": "and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024)."}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_3", "claim_id": "gemma2_sample_3#1", "claim_text": "Increasing the length of training leads to performance gains in small models.", "source_quote": "These gains are largely derived from increasing the length of training"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_3", "claim_id": "gemma2_sample_3#2", "claim_text": "The performance scaling of small models is logarithmic with respect to dataset size.", "source_quote": "This approach only scales logarithmically with dataset size"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_3", "claim_id": "gemma2_sample_3#3", "claim_text": "The latest small models require up to 15T tokens to improve the state of the art by less than 1-2%.", "source_quote": "require up to 15T tokens to improve the state of the art by less than 1-2%"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_3", "claim_id": "gemma2_sample_3#4", "claim_text": "Small models are still under-trained.", "source_quote": "Yet, these continued improvements provide evidence that small models are still under-trained"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_3", "claim_id": "gemma2_sample_3#5", "claim_text": "Knowledge distillation replaces the next token prediction task.", "source_quote": "which replaces the next token prediction task"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_4", "claim_id": "gemma2_sample_4#1", "claim_text": "Knowledge distillation replaces the one-hot vector with the distribution of potential next tokens.", "source_quote": "which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model."}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_4", "claim_id": "gemma2_sample_4#2", "claim_text": "Knowledge distillation is used to reduce the training time of smaller models.", "source_quote": "This approach is often used to reduce the training time of smaller models by giving them richer gradients."}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_4", "claim_id": "gemma2_sample_4#3", "claim_text": "The models are trained on a quantity of tokens that is more than 50× the compute-optimal quantity.", "source_quote": "on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory"}
{"doc": "gemma2_sample", "chunk_id": "gemma2_sample_4", "claim_id": "gemma2_sample_4#4", "claim_text": "The models include a 27B model trained from scratch.", "source_quote": "Along with the models trained with distillation, we also release a 27B model trained from scratch for this work."}