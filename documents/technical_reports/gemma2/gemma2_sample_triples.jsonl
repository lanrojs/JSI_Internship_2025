{"claim_id": "gemma2_sample_1#1", "claim_text": "Gemma 2 models range in scale from 2 billion to 27 billion parameters.", "subject": "Gemma 2 models", "predicate": "range in scale from", "object": "2 billion to 27 billion parameters"}
{"claim_id": "gemma2_sample_1#2", "claim_text": "Gemma 2 applies interleaving local-global attentions (Beltagy et al., 2020a).", "subject": "Gemma 2", "predicate": "applies", "object": "interleaving local-global attentions"}
{"claim_id": "gemma2_sample_1#3", "claim_text": "Gemma 2 applies group-query attention (Ainslie et al., 2023).", "subject": "Gemma 2", "predicate": "applies", "object": "group-query attention (Ainslie et al., 2023)"}
{"claim_id": "gemma2_sample_1#4", "claim_text": "The 2B and 9B models are trained with knowledge distillation (Hinton et al., 2015).", "subject": "2B and 9B models", "predicate": "are trained with", "object": "knowledge distillation"}
{"claim_id": "gemma2_sample_1#5", "claim_text": "The resulting models deliver the best performance for their size.", "subject": "resulting models", "predicate": "deliver", "object": "the best performance for their size"}
{"claim_id": "gemma2_sample_1#6", "claim_text": "Gemma 2 models are released to the community.", "subject": "Gemma 2 models", "predicate": "are released", "object": "to the community"}
{"claim_id": "gemma2_sample_2#1", "claim_text": "Models that are 2-3× bigger have demonstrated unprecedented performance on reasoning benchmarks.", "subject": "models that are 2-3× bigger", "predicate": "have demonstrated", "object": "unprecedented performance on reasoning benchmarks"}
{"claim_id": "gemma2_sample_2#2", "claim_text": "Newest large models demonstrate multimodal capabilities.", "subject": "newest large models", "predicate": "demonstrate", "object": "multimodal capabilities"}
{"claim_id": "gemma2_sample_2#3", "claim_text": "Newest large models demonstrate multilingual capabilities.", "subject": "newest large models", "predicate": "demonstrate", "object": "multilingual capabilities"}
{"claim_id": "gemma2_sample_2#4", "claim_text": "Newest large models can use context lengths of over 1M tokens.", "subject": "newest large models", "predicate": "can use", "object": "context lengths of over 1M tokens"}
{"claim_id": "gemma2_sample_3#1", "claim_text": "Increasing the length of training leads to performance gains in small models.", "subject": "increasing the length of training", "predicate": "leads to", "object": "performance gains in small models"}
{"claim_id": "gemma2_sample_3#2", "claim_text": "The performance scaling of small models is logarithmic with respect to dataset size.", "subject": "performance scaling of small models", "predicate": "is", "object": "logarithmic with respect to dataset size"}
{"claim_id": "gemma2_sample_3#3", "claim_text": "The latest small models require up to 15T tokens to improve the state of the art by less than 1-2%.", "subject": "latest small models", "predicate": "require", "object": "up to 15T tokens to improve the state of the art by less than 1-2%"}
{"claim_id": "gemma2_sample_3#4", "claim_text": "Small models are still under-trained.", "subject": "small models", "predicate": "are still under-trained", "object": ""}
{"claim_id": "gemma2_sample_3#5", "claim_text": "Knowledge distillation replaces the next token prediction task.", "subject": "knowledge distillation", "predicate": "replaces", "object": "the next token prediction task"}
{"claim_id": "gemma2_sample_4#1", "claim_text": "Knowledge distillation replaces the one-hot vector with the distribution of potential next tokens.", "subject": "knowledge distillation", "predicate": "replaces", "object": "one-hot vector with the distribution of potential next tokens"}
{"claim_id": "gemma2_sample_4#2", "claim_text": "Knowledge distillation is used to reduce the training time of smaller models.", "subject": "knowledge distillation", "predicate": "is used to reduce", "object": "training time of smaller models"}
{"claim_id": "gemma2_sample_4#3", "claim_text": "The models are trained on a quantity of tokens that is more than 50× the compute-optimal quantity.", "subject": "models", "predicate": "are trained", "object": "a quantity of tokens that is more than 50× the compute-optimal quantity"}
{"claim_id": "gemma2_sample_4#4", "claim_text": "The models include a 27B model trained from scratch.", "subject": "models", "predicate": "include", "object": "a 27B model trained from scratch"}
