[
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_1",
    "chunk": "Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning",
    "context_used": {
      "local": "models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;",
      "doc": "(no retrieved context)",
      "system": "(no system context)",
      "global": "This passage details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. The document likely outlines the technical improvements and performance characteristics of this model relative to existing, larger models, aiming to contribute to ongoing research and development in the area.",
      "flags": {
        "keep_local": true,
        "keep_doc": false,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_2",
    "chunk": "models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;",
    "context_used": {
      "local": "Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning\n\nSmall-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning",
      "system": "(no system context)",
      "global": "This passage describes the development and release of Gemma 2, a new family of open-source language models.  It’s part of a larger effort to improve the performance of smaller models, positioning Gemma 2 as a competitive alternative to larger models while addressing limitations in current small-scale LLM training methods.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_3",
    "chunk": "Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot",
    "context_used": {
      "local": "Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning\n\nmodels that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;\n\nby replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning",
      "system": "(no system context)",
      "global": "This passage discusses research focused on improving the performance of small language models. Specifically, it investigates methods beyond simply increasing training length, centering on utilizing knowledge distillation to provide richer training data and simulate larger training datasets. The work is situated within a broader effort to develop and release a family of lightweight, open-source language models, Gemma 2, and its associated research.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_4",
    "chunk": "by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Specifically, the research focuses on using knowledge distillation – a technique where a smaller model learns from a larger one – to improve the performance of these models, simulating training with a quantity of tokens far exceeding what would be computationally optimal.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Specifically, the research focuses on using knowledge distillation – a technique where a smaller model learns from a larger one – to improve the performance of these models, simulating training with a quantity of tokens far exceeding what would be computationally optimal. by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the",
    "context_used": {
      "local": "models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;\n\nSmall-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot\n\nal., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;",
      "system": "(no system context)",
      "global": "This passage describes research focused on enhancing the performance of smaller language models. The work investigates techniques like knowledge distillation to overcome limitations in training data, aiming to improve these models’ capabilities beyond what’s achievable with simply increasing training length – a strategy that ultimately scales logarithmically. The research is presented as part of a larger effort to release advanced open-source models and associated techniques for the broader AI community.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_5",
    "chunk": "al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,",
    "context_used": {
      "local": "Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot\n\nby replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the\n\nand human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Specifically, the research focuses on using knowledge distillation – a technique where a smaller model learns from a larger one – to improve the performance of these models, simulating training with a quantity of tokens far exceeding what would be computationally optimal. by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot",
      "system": "(no system context)",
      "global": "This passage details the development and evaluation of Gemma 2, a series of language models focused on achieving high performance despite their relatively small size. It’s presented within a larger document likely outlining the technical specifications and performance benchmarks of these models, positioning them as an advancement in open-source language model technology.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_6",
    "chunk": "and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. Gemma 2 is a new, smaller language model released by Google DeepMind, presented as an advancement within the field of open-source large language models.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. Gemma 2 is a new, smaller language model released by Google DeepMind, presented as an advancement within the field of open-source large language models. and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672",
    "context_used": {
      "local": "by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the\n\nal., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,\n\n4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning (Brown et al., 2020; Radford et al., 2019; Raffel et al., 2019). Scaling has been key to this recent progress, with many new capabilities only emerging at scale (Brown et al., 2020). The newest large models not only reach unprecedented performance on reasoning benchmarks (Achiam et al., 2023), but they also demonstrate multimodal and multilingual capabilities (Gemini Team, 2024) and even the ability to use context lengths of over 1M tokens (Gemini Team, 2024). Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023;\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning",
      "system": "(no system context)",
      "global": "This passage details the specifications of several large language models, Gemma 2, 2B, 9B, and 27B, focusing on their architectural choices and training methodologies. It’s part of a technical report intended to provide a comprehensive overview of these models and their development, alongside guidance for users regarding responsible deployment.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_7",
    "chunk": "4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including",
    "context_used": {
      "local": "al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,\n\nand human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672\n\nbe used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages. 2. Model Architecture Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. Gemma 2 is a new, smaller language model released by Google DeepMind, presented as an advancement within the field of open-source large language models. and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Specifically, the research focuses on using knowledge distillation – a technique where a smaller model learns from a larger one – to improve the performance of these models, simulating training with a quantity of tokens far exceeding what would be computationally optimal. by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the",
      "system": "(no system context)",
      "global": "This passage details the technical specifications of the Gemma 2 language model, including its architecture, training parameters, and design choices. It’s presented as part of a broader technical report that evaluates the model’s performance, safety considerations, and deployment guidelines, aiming to provide comprehensive information for users and researchers.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_8",
    "chunk": "be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages. 2. Model Architecture Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages. 2. Model Architecture Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters",
    "context_used": {
      "local": "and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672\n\n4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including\n\n2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters Non-embedding Parameters 2B 590,118,912 2,024,517,888 9B 917,962,752 8,324,201,984 27B 1,180,237,824 26,047,480,320 Table 2 | Parameter counts for the Gemma models. We inherit from the large Gemini vocabulary (256k entries), that is designed to work on a large number of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages. length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Small-scale models have also shown a rapid increase in performance, but these gains are largely derived from increasing the length of training (Gemma Team, 2024; Jiang et al., 2023; Touvron et al., 2023). This approach only scales logarithmically with dataset size (Hoffmann et al., 2022), and the latest small models require up to 15T tokens to improve the state of the art by less than 1-2% (AI@Meta, 2024). Yet, these continued improvements provide evidence that small models are still under-trained. In this work, we explore alternatives to improve small model performance without solely increasing training length. One solution is to improve the quality of information received by the network at each training step by replacing the next token prediction task with a richer objective. In particular, we focus our efforts on knowledge distillation (Hinton et al., 2015), which replaces the one-hot vector seen at each token with the distribution of potential next tokens computed from a large model. This approach is often used to reduce the training time of smaller models by giving them richer gradients. In this work, we instead train for large quantities of tokens with distillation in order to simulate training beyond the number of available tokens. Concretely, we use a large language model as a teacher to train small models, namely 2B and 9B models, on a quantity of tokens that is more than 50× the compute-optimal quantity predicted by the theory (Hoffmann et al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., al., 2022). Along with the models trained with distillation, we also release a 27B model trained from scratch for this work. We also leverage several known modifications of Transformers, namely the interleaving of global and local attention layers from Beltagy et al. (2020a), and the Grouped-Query Attention (GQA) mechanism of Ainslie et al. (2023). Overall, Gemma 2 significantly advances stateof-the-art performance relative to comparablescale open models and are even competitive with some models more than twice their size (AI@Meta, 2024; Almazrouei et al., 2023; Jiang et al., 2023; xAI, 2024), across a variety of automated benchmarks and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al.,\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. Gemma 2: Improving Open Language Models at a Practical Size Gemma Team, Google DeepMind1 In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community. 1. Introduction Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning",
      "system": "(no system context)",
      "global": "This passage details the specifications of the Gemma 2 language model, including its architecture, training, and key parameters. It’s part of a technical report intended to provide a comprehensive overview of the model for users, emphasizing the need for tailored safety testing before deployment due to the model’s broad potential applications.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  },
  {
    "doc": "gemma2_sample",
    "id": "gemma2_sample_9",
    "chunk": "2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters Non-embedding Parameters 2B 590,118,912 2,024,517,888 9B 917,962,752 8,324,201,984 27B 1,180,237,824 26,047,480,320 Table 2 | Parameter counts for the Gemma models. We inherit from the large Gemini vocabulary (256k entries), that is designed to work on a large number of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages. length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity",
    "context_prefix": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. Gemma 2 is a new, smaller language model released by Google DeepMind, presented as an advancement within the field of open-source large language models.",
    "contextualized_chunk": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. Gemma 2 is a new, smaller language model released by Google DeepMind, presented as an advancement within the field of open-source large language models. 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters Non-embedding Parameters 2B 590,118,912 2,024,517,888 9B 917,962,752 8,324,201,984 27B 1,180,237,824 26,047,480,320 Table 2 | Parameter counts for the Gemma models. We inherit from the large Gemini vocabulary (256k entries), that is designed to work on a large number of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages. length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity",
    "context_used": {
      "local": "4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including\n\nbe used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages. 2. Model Architecture Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters",
      "doc": "This passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including the architecture, training, and pre- and post-training recipes for Gemma 2. We also provide detailed evaluations across a wide variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. Finally, we discuss our approach to safe and responsible deployment and outline the broader implications of Gemma 2, its limitations, and advantages. 2. Model Architecture Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1. A few architectural elements are similar to the first version of Gemma models; namely, a context Model Embedding Parameters\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. The document details the release of a new, smaller language model called Gemma 2, presented as an advancement within the broader field of open-source large language models. 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672 73728 Head type GQA GQA GQA Num heads 8 16 32 Num KV heads 4 8 16 Head size 256 256 128 Global att. span 8192 8192 8192 Sliding window 4096 4096 4096 Vocab size 256128 256128 256128 Tied embedding yes yes yes Table 1 | Overview of the main model parameters and design choices. See the section on model architectures for more details. While thorough testing of our models has been conducted, these tests cannot cover all applications and scenarios in which Gemma 2 may be used. With this in mind, all Gemma 2 users should conduct rigorous safety testing specific to their use case before deployment or use. In this technical report, we provide an overview of models, including\n\n---\n\nThis passage appears in Section 1 (Introduction) of the technical report *Gemma 2: Improving Open Language Models at a Practical Size* by Google DeepMind. “Parameters” refers to the numerical values within a model that are adjusted during training, and “knowledge distillation” is a training technique where a smaller model learns from a larger, more complex one. Gemma 2 is a new, smaller language model released by Google DeepMind, presented as an advancement within the field of open-source large language models. and human evaluations. Example domains include question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), mathematics and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021). 1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-2-report@google.com. © 2024 Google DeepMind. All rights reserved Gemma 2: Improving Open Language Models at a Practical Size Parameters 2B 9B 27B d_model 2304 3584 4608 Layers 26 42 46 Pre-norm yes yes yes Post-norm yes yes yes Non-linearity GeGLU GeGLU GeGLU Feedforward dim 18432 28672",
      "system": "(no system context)",
      "global": "This passage details the specific architectural choices and parameter counts for the Gemma 2 models, presented as part of a broader technical report that outlines the models’ design, training, and evaluation. The report aims to provide a comprehensive overview of Gemma 2, including its limitations and responsible deployment considerations.",
      "flags": {
        "keep_local": true,
        "keep_doc": true,
        "keep_system": false,
        "keep_global": true
      }
    }
  }
]