[
  {
    "doc": "llama3_sample",
    "id": "llama3_sample_1",
    "chunk": "Introduction Foundation models are general models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks. They form the basis of many modern AI systems. The development of modern foundation models consists of two main stages: (1) a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next-word prediction or captioning and (2) a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities (for example, coding and reasoning). In this paper, we present a new set of foundation models for language, called Llama 3. The Llama 3 Herd of models natively supports multilinguality, coding, reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. Each member of the herd is listed in Table",
    "contextualized_chunk": "This passage is from a research paper detailing the development of the Llama 3 foundation models. The paper outlines a two-stage development process: pre-training at massive scale using tasks like next-word prediction, and post-training to improve instruction following and specific capabilities. The Llama 3 Herd includes models with up to 405 billion parameters, capable of processing context windows of 128,000 tokens. Introduction Foundation models are general models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks. They form the basis of many modern AI systems. The development of modern foundation models consists of two main stages: (1) a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next-word prediction or captioning and (2) a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities (for example, coding and reasoning). In this paper, we present a new set of foundation models for language, called Llama 3. The Llama 3 Herd of models natively supports multilinguality, coding, reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. Each member of the herd is listed in Table"
  },
  {
    "doc": "llama3_sample",
    "id": "llama3_sample_2",
    "chunk": "reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. Each member of the herd is listed in Table 1. All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity. We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process: • Data. Compared to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity and quality of the data we use for pre-training and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data and the development of more rigorous quality assurance and filtering approaches for post-training",
    "contextualized_chunk": "This passage comes from a research paper detailing the development of the Llama 3 foundation models. The paper outlines a two-stage development process: pre-training at massive scale using tasks like next-word prediction, and post-training to improve instruction following and specific capabilities. The Llama 3 Herd includes models with 405 billion parameters, capable of processing context windows of 128,000 tokens. reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens. Each member of the herd is listed in Table 1. All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity. We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process: • Data. Compared to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity and quality of the data we use for pre-training and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data and the development of more rigorous quality assurance and filtering approaches for post-training"
  },
  {
    "doc": "llama3_sample",
    "id": "llama3_sample_3",
    "chunk": "post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data and the development of more rigorous quality assurance and filtering approaches for post-training data. We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2. • Scale. We train a model at far larger scale than previous Llama models: our flagship language model was pre-trained using 3.8 × 1025 FLOPs, almost 50× more than the largest version of Llama 2. Specifically, we pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens. As expected per 1 arXiv:2407.21783v3 [cs.AI] 23 Nov 2024 Finetuned Multilingual Long context Tool use Release Llama 3 8B ✗ ✗ 1 ✗ ✗ April 2024 Llama 3",
    "contextualized_chunk": "This passage is from a research paper detailing the development of the Llama 3 foundation models. The paper outlines a two-stage development process: pre-training at massive scale using tasks like next-word prediction, and post-training to improve instruction following and specific capabilities. The Llama 3 Herd includes models with up to 405 billion parameters, capable of processing context windows of 128,000 tokens. post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data and the development of more rigorous quality assurance and filtering approaches for post-training data. We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2. • Scale. We train a model at far larger scale than previous Llama models: our flagship language model was pre-trained using 3.8 × 1025 FLOPs, almost 50× more than the largest version of Llama 2. Specifically, we pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens. As expected per 1 arXiv:2407.21783v3 [cs.AI] 23 Nov 2024 Finetuned Multilingual Long context Tool use Release Llama 3 8B ✗ ✗ 1 ✗ ✗ April 2024 Llama 3"
  },
  {
    "doc": "llama3_sample",
    "id": "llama3_sample_4",
    "chunk": "arXiv:2407.21783v3 [cs.AI] 23 Nov 2024 Finetuned Multilingual Long context Tool use Release Llama 3 8B ✗ ✗ 1 ✗ ✗ April 2024 Llama 3 8B Instruct ✓ ✗ ✗ ✗ April 2024 Llama 3 70B ✗ ✗ 1 ✗ ✗ April 2024 Llama 3 70B Instruct ✓ ✗ ✗ ✗ April 2024 Llama 3.1 8B ✗ ✓ ✓ ✗ July 2024 Llama 3.1 8B Instruct ✓ ✓ ✓ ✓ July 2024 Llama 3.1 70B ✗ ✓ ✓ ✗ July 2024 Llama 3.1 70B Instruct ✓ ✓ ✓ ✓ July 2024 Llama 3.1 405B ✗ ✓ ✓ ✗ July 2024 Llama 3.1 405B Instruct ✓ ✓ ✓ ✓ July 2024 Table 1 Overview of the Llama 3 Herd of models. All results in this paper are for the Llama",
    "contextualized_chunk": "This passage is from a research paper detailing the development of the Llama 3 foundation models. The Llama 3 Herd includes models with up to 405 billion parameters, capable of processing context windows of 128,000 tokens. All results in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity. arXiv:2407.21783v3 [cs.AI] 23 Nov 2024 Finetuned Multilingual Long context Tool use Release Llama 3 8B ✗ ✗ 1 ✗ ✗ April 2024 Llama 3 8B Instruct ✓ ✗ ✗ ✗ April 2024 Llama 3 70B ✗ ✗ 1 ✗ ✗ April 2024 Llama 3 70B Instruct ✓ ✗ ✗ ✗ April 2024 Llama 3.1 8B ✗ ✓ ✓ ✗ July 2024 Llama 3.1 8B Instruct ✓ ✓ ✓ ✓ July 2024 Llama 3.1 70B ✗ ✓ ✓ ✗ July 2024 Llama 3.1 70B Instruct ✓ ✓ ✓ ✓ July 2024 Llama 3.1 405B ✗ ✓ ✓ ✗ July 2024 Llama 3.1 405B Instruct ✓ ✓ ✓ ✓ July 2024 Table 1 Overview of the Llama 3 Herd of models. All results in this paper are for the Llama"
  }
]