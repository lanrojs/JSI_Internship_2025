{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#1", "claim_text": "The development of modern foundation models consists of a pre-training stage.", "source_quote": "The development of modern foundation models consists of two main stages: (1) a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next-word prediction or captioning and (2) a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#2", "claim_text": "The pre-training stage involves training the model at massive scale using next-word prediction or captioning.", "source_quote": "a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next-word prediction or captioning"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#3", "claim_text": "The development of modern foundation models consists of a post-training stage.", "source_quote": "a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#4", "claim_text": "The post-training stage involves tuning the model to follow instructions, align with human preferences, and improve specific capabilities.", "source_quote": "a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#5", "claim_text": "The Llama 3 Herd natively supports multilinguality.", "source_quote": "The Llama 3 Herd of models natively supports multilinguality"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#6", "claim_text": "The Llama 3 Herd natively supports coding.", "source_quote": "The Llama 3 Herd of models natively supports coding"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#7", "claim_text": "The Llama 3 Herd natively supports reasoning.", "source_quote": "The Llama 3 Herd of models natively supports reasoning"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#8", "claim_text": "The Llama 3 Herd natively supports tool usage.", "source_quote": "The Llama 3 Herd of models natively supports tool usage"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#9", "claim_text": "The largest Llama 3 model has 405 billion parameters.", "source_quote": "Our largest model is dense Transformer with 405B parameters"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_1", "claim_id": "llama3_sample_1#10", "claim_text": "The largest Llama 3 model processes information in a context window of up to 128,000 tokens.", "source_quote": "processing information in a context window of up to 128K tokens"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_2", "claim_id": "llama3_sample_2#1", "claim_text": "The Llama 3 models have 405 billion parameters.", "source_quote": "Our largest model is dense Transformer with 405B parameters"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_2", "claim_id": "llama3_sample_2#2", "claim_text": "The Llama 3 models can process information in a context window of up to 128,000 tokens.", "source_quote": "processing information in a context window of up to 128K tokens"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_2", "claim_id": "llama3_sample_2#3", "claim_text": "The data used for pre-training and post-training was improved compared to prior versions of Llama.", "source_quote": "Compared to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity and quality of the data we use for pre-training and post-training"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_3", "claim_id": "llama3_sample_3#1", "claim_text": "Llama 3 was pre-trained on approximately 15 trillion multilingual tokens.", "source_quote": "We pre-train Llama 3 on a corpus of about 15T multilingual tokens"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_3", "claim_id": "llama3_sample_3#2", "claim_text": "Llama 3 was pre-trained with approximately 3.8 × 10^25 FLOPs.", "source_quote": "our flagship language model was pre-trained using 3.8 × 1025 FLOPs"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_3", "claim_id": "llama3_sample_3#3", "claim_text": "Llama 3 has 405 billion trainable parameters.", "source_quote": "Specifically, we pre-trained a flagship model with 405B trainable parameters"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_3", "claim_id": "llama3_sample_3#4", "claim_text": "Llama 2 was pre-trained on approximately 1.8 trillion tokens.", "source_quote": "compared to 1.8T tokens for Llama 2"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_4", "claim_id": "llama3_sample_4#1", "claim_text": "Llama 3 8B was released in April 2024.", "source_quote": "Llama 3 8B ✗ ✗ 1 ✗ ✗ April 2024"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_4", "claim_id": "llama3_sample_4#2", "claim_text": "Llama 3 70B was released in April 2024.", "source_quote": "Llama 3 70B ✗ ✗ 1 ✗ ✗ April 2024"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_4", "claim_id": "llama3_sample_4#3", "claim_text": "Llama 3.1 8B was released in July 2024.", "source_quote": "Llama 3.1 8B ✗ ✓ ✓ ✗ July 2024"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_4", "claim_id": "llama3_sample_4#4", "claim_text": "Llama 3.1 70B was released in July 2024.", "source_quote": "Llama 3.1 70B ✗ ✓ ✓ ✗ July 2024"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_4", "claim_id": "llama3_sample_4#5", "claim_text": "Llama 3.1 405B was released in July 2024.", "source_quote": "Llama 3.1 405B ✗ ✓ ✓ ✗ July 2024"}
{"doc": "llama3_sample", "chunk_id": "llama3_sample_4", "claim_id": "llama3_sample_4#6", "claim_text": "All results in this paper are for the Llama.", "source_quote": "All results in this paper are for the Llama"}
